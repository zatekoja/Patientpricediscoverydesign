# LLM Tag Generator Provider

External data provider that enriches healthcare price data with contextual tags generated by Large Language Models (LLMs) to power semantic search in Typesense.

## Overview

The `LLMTagGeneratorProvider` is a specialized external data provider that:

1. **Receives price data** from upstream providers (e.g., Google Sheets)
2. **Generates contextual tags** using an LLM (GPT-4, Claude, etc.)
3. **Enriches the data** with searchable metadata
4. **Stores tagged data** in a document store
5. **Enables semantic search** in Typesense or other search engines

## Architecture

```
┌─────────────────────┐
│  Google Sheets      │
│  Provider           │
└──────────┬──────────┘
           │ Raw price data
           ▼
┌─────────────────────┐
│  LLM Tag Generator  │
│  Provider           │
│                     │
│  ┌──────────────┐   │
│  │ LLM API      │   │ Generates tags:
│  │ (GPT-4, etc) │   │ - Procedure type
│  └──────────────┘   │ - Body part/system
│                     │ - Medical specialty
└──────────┬──────────┘ - Related conditions
           │ Tagged data   - Symptoms
           ▼
┌─────────────────────┐
│  Document Store     │
│  (S3/Dynamo/Mongo)  │
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  Typesense Search   │
│  Engine             │
└─────────────────────┘
```

## Usage

### Basic Setup

```typescript
import { MegalekAteruHelper } from './providers/MegalekAteruHelper';
import { LLMTagGeneratorProvider } from './providers/LLMTagGeneratorProvider';
import { InMemoryDocumentStore } from './stores/InMemoryDocumentStore';

// 1. Create stores
const rawDataStore = new InMemoryDocumentStore<PriceData>('raw-data');
const taggedDataStore = new InMemoryDocumentStore<TaggedPriceData>('tagged-data');

// 2. Setup source provider (Google Sheets)
const sheetsProvider = new MegalekAteruHelper(rawDataStore);
await sheetsProvider.initialize({
  credentials: { /* Google credentials */ },
  spreadsheetIds: ['spreadsheet-id'],
});

// 3. Setup LLM provider
const llmProvider = new LLMTagGeneratorProvider(taggedDataStore, sheetsProvider);
await llmProvider.initialize({
  apiEndpoint: 'https://api.openai.com/v1/chat/completions',
  apiKey: process.env.LLM_API_KEY,
  model: 'gpt-4',
  maxTags: 10,
  temperature: 0.3,
});

// 4. Sync data through the pipeline
await sheetsProvider.syncData();  // Fetch from Google Sheets
await llmProvider.syncData();     // Add LLM tags
```

### Configuration

```typescript
interface LLMTagGeneratorConfig {
  /** LLM API endpoint URL */
  apiEndpoint: string;
  
  /** API key for authentication */
  apiKey: string;
  
  /** LLM model (e.g., "gpt-4", "claude-3-opus") */
  model: string;
  
  /** System prompt for tag generation (optional) */
  systemPrompt?: string;
  
  /** Maximum tags per item (default: 10) */
  maxTags?: number;
  
  /** Temperature for generation (default: 0.3) */
  temperature?: number;
}
```

### Environment Variables

```bash
# LLM Configuration
LLM_API_ENDPOINT=https://api.openai.com/v1/chat/completions
LLM_API_KEY=sk-...
LLM_MODEL=gpt-4
```

## Ingestion Workflow

The complete ingestion workflow integrates all components:

```typescript
import { setupIngestionWorkflow } from './workflows/ingestion-with-llm';

// Setup the workflow
const { googleSheetsProvider, llmProvider, scheduler } = await setupIngestionWorkflow();

// Workflow runs automatically:
// 1. Every 3 days, Google Sheets provider syncs data
// 2. On successful sync, LLM provider generates tags
// 3. Tagged data is indexed in Typesense
```

See `workflows/ingestion-with-llm.ts` for the complete implementation.

## Tag Generation

The LLM generates contextual tags based on:

- **Procedure type**: "imaging", "surgery", "diagnostic test"
- **Body part**: "brain", "knee", "heart"
- **Medical specialty**: "cardiology", "orthopedics", "radiology"
- **Conditions**: "fracture", "infection", "chronic pain"
- **Symptoms**: "headache", "joint pain", "chest pain"
- **Procedure category**: "outpatient", "inpatient", "emergency"

### Example

**Input:**
```json
{
  "facilityName": "General Hospital",
  "procedureDescription": "CT Scan - Head without contrast",
  "procedureCode": "70450",
  "price": 1250.00
}
```

**LLM-Generated Tags:**
```json
{
  "tags": [
    "imaging",
    "ct scan",
    "head",
    "brain",
    "diagnostic",
    "radiology",
    "headache",
    "neurological",
    "outpatient",
    "non-invasive"
  ],
  "tagMetadata": {
    "generatedAt": "2024-01-15T10:30:00Z",
    "model": "gpt-4"
  }
}
```

## Contextual Search Examples

With LLM-generated tags, users can search using natural language:

```typescript
// Search in Typesense
const results = await typesense.search({
  q: 'brain imaging',
  query_by: 'procedureDescription,tags',
  filter_by: 'price:<2000'
});
```

**Search Queries Enabled:**

1. **"brain imaging"** → Finds CT scans, MRIs of the head
2. **"knee pain"** → Finds procedures for knee injuries
3. **"diagnostic tests"** → Finds all diagnostic procedures
4. **"cardiology procedures"** → Finds heart-related services
5. **"outpatient imaging"** → Finds imaging that's typically outpatient

## Integration with Typesense

After tagging, index the data in Typesense:

```typescript
import Typesense from 'typesense';

const client = new Typesense.Client({
  nodes: [{ host: 'localhost', port: '8108', protocol: 'http' }],
  apiKey: process.env.TYPESENSE_API_KEY,
});

// Define schema with tags field
const schema = {
  name: 'healthcare_prices',
  fields: [
    { name: 'id', type: 'string' },
    { name: 'procedureDescription', type: 'string' },
    { name: 'tags', type: 'string[]', facet: true },
    { name: 'price', type: 'float', facet: true },
    // ... other fields
  ],
};

await client.collections().create(schema);

// Index tagged documents
const taggedData = await llmProvider.getCurrentData();
await client.collections('healthcare_prices').documents().import(taggedData.data);
```

## Custom LLM Providers

The provider supports any LLM with an OpenAI-compatible API:

### OpenAI

```typescript
{
  apiEndpoint: 'https://api.openai.com/v1/chat/completions',
  apiKey: process.env.OPENAI_API_KEY,
  model: 'gpt-4',
}
```

### Anthropic Claude

```typescript
{
  apiEndpoint: 'https://api.anthropic.com/v1/messages',
  apiKey: process.env.ANTHROPIC_API_KEY,
  model: 'claude-3-opus-20240229',
}
```

### Azure OpenAI

```typescript
{
  apiEndpoint: 'https://your-resource.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-02-15-preview',
  apiKey: process.env.AZURE_OPENAI_KEY,
  model: 'gpt-4',
}
```

### Self-hosted LLM

```typescript
{
  apiEndpoint: 'http://localhost:11434/v1/chat/completions',
  apiKey: 'not-needed',
  model: 'llama3',
}
```

## Customizing Tag Generation

### Custom System Prompt

```typescript
const customPrompt = `You are a medical billing expert.
Generate tags that help patients find affordable procedures.
Focus on: procedure type, medical specialty, common use cases.`;

await llmProvider.initialize({
  // ... other config
  systemPrompt: customPrompt,
});
```

### Adjusting Tag Count

```typescript
await llmProvider.initialize({
  // ... other config
  maxTags: 15, // Generate up to 15 tags per item
});
```

### Controlling Creativity

```typescript
await llmProvider.initialize({
  // ... other config
  temperature: 0.1, // Very focused tags (less creative)
  // temperature: 0.5, // Balanced
  // temperature: 0.9, // More diverse tags
});
```

## Performance Considerations

### Batch Processing

The provider processes items in batches of 10 to avoid rate limits:

```typescript
// Automatically batches requests
await llmProvider.syncData(); // Processes 1000 items in batches of 10
```

### Caching

Tags are cached in the document store to avoid regenerating:

```typescript
// Tags are only generated once per unique key
const key = `${provider}_${facility}_${procedure}_${date}`;
```

### Rate Limiting

Implement rate limiting for production:

```typescript
// Add delay between batches
const BATCH_DELAY_MS = 1000; // 1 second between batches
```

## Monitoring

Track tag generation metrics:

```typescript
scheduler.scheduleJob({
  name: 'llm-tagging',
  provider: llmProvider,
  intervalMs: SyncIntervals.THREE_DAYS,
  onComplete: (result) => {
    console.log('Tag generation metrics:', {
      recordsProcessed: result.recordsProcessed,
      success: result.success,
      timestamp: result.timestamp,
    });
    
    // Send to monitoring service
    // metrics.track('llm_tagging_complete', result);
  },
});
```

## Cost Optimization

LLM API calls can be expensive. Optimize costs by:

1. **Only tag new/updated items** - Check if tags already exist
2. **Use cheaper models for simple tagging** - GPT-3.5 vs GPT-4
3. **Batch requests** - Send multiple items per API call
4. **Cache aggressively** - Store tags with stable keys
5. **Use local LLMs** - Self-host Llama or Mistral models

## Next Steps

1. **Implement LLM API Integration** - Replace placeholder with actual API calls
2. **Add Typesense Integration** - Index tagged data for search
3. **Set up Monitoring** - Track tag quality and costs
4. **Optimize Prompts** - Test and refine tag generation prompts
5. **Add Tag Validation** - Filter inappropriate or irrelevant tags

## Files

- `providers/LLMTagGeneratorProvider.ts` - Main provider implementation
- `workflows/ingestion-with-llm.ts` - Complete ingestion workflow
- `workflows/README.md` - This file

## Support

For issues or questions about LLM tag generation:
1. Check LLM API documentation (OpenAI, Anthropic, etc.)
2. Review prompt engineering best practices
3. Monitor tag quality in Typesense search results
